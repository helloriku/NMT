{"cells":[{"metadata":{"id":"UXkBSpVSK6gi","outputId":"26c1b83c-4af9-4cc1-bb69-2eac81172ec9","trusted":true},"cell_type":"code","source":"# %tensorflow_version 2.x\nimport tensorflow as tf\nprint(\"Tensorflow version \" + tf.__version__)\ntry:\n  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\nexcept ValueError:\n  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","execution_count":1,"outputs":[{"output_type":"stream","text":"Tensorflow version 2.2.0\nRunning on TPU  ['10.0.0.2:8470']\n","name":"stdout"}]},{"metadata":{"trusted":true,"id":"oZdsxDYmK5KA","outputId":"d7f8b4ea-28e8-4806-e462-8b55a0d8e67f"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nimport time\nimport matplotlib.pyplot as plt\nfrom nltk.translate.bleu_score import sentence_bleu\nimport math\nprint(tf.__version__)\nAUTO = tf.data.experimental.AUTOTUNE","execution_count":2,"outputs":[{"output_type":"stream","text":"2.2.0\n","name":"stdout"}]},{"metadata":{"id":"VKSbVMtKMd7N","outputId":"1b4b5396-e6c1-4432-8c21-efab28262408","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndata_dir = dirname","execution_count":3,"outputs":[{"output_type":"stream","text":"/kaggle/input/chineseenglish-mt/dev.zh\n/kaggle/input/chineseenglish-mt/dev.en\n/kaggle/input/chineseenglish-mt/test.zh\n/kaggle/input/chineseenglish-mt/train.en\n/kaggle/input/chineseenglish-mt/train.zh\n/kaggle/input/chineseenglish-mt/vocab.en\n/kaggle/input/chineseenglish-mt/test.en\n/kaggle/input/chineseenglish-mt/vocab.zh\n","name":"stdout"}]},{"metadata":{"id":"WTTtGWKXK5KM"},"cell_type":"markdown","source":"# 构建词汇表"},{"metadata":{"trusted":true,"id":"OW146DbpK5KN"},"cell_type":"code","source":"\n# 数据集已预处理好词汇表\ndef build_wordlist(path):\n    word2id = {}\n    id2word = {}\n    with open(path, encoding = 'utf-8') as f:\n        for line,word in enumerate(f):\n            word2id[word.strip()] = line\n            id2word[line] = word.strip()\n    return word2id,id2word\n    ","execution_count":4,"outputs":[]},{"metadata":{"trusted":true,"id":"gcixr8sKK5KQ"},"cell_type":"code","source":"word2id_zh, id2word_zh = build_wordlist(data_dir + '/vocab.zh')\nword2id_en, id2word_en = build_wordlist(data_dir + '/vocab.en')","execution_count":5,"outputs":[]},{"metadata":{"trusted":true,"id":"exEBgBnsK5KT","outputId":"60135e1e-cf7e-4ddd-b536-7ad6b00fe6e8"},"cell_type":"code","source":"len(word2id_zh)","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"53712"},"metadata":{}}]},{"metadata":{"id":"FbEDmXfuLlim","outputId":"10c35563-924c-42fd-c719-472de1fa83ca","trusted":true},"cell_type":"code","source":"len(word2id_en)","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"35028"},"metadata":{}}]},{"metadata":{"id":"yqAekFrpK5KX"},"cell_type":"markdown","source":"# 构建训练集"},{"metadata":{"id":"jgq0zzhGK5KX"},"cell_type":"markdown","source":"## 统计样本长度分布"},{"metadata":{"trusted":true,"id":"iXs799gKK5KY"},"cell_type":"code","source":"#统计训练样本词汇个数分布\ndef words_count(path):\n    num_words = []\n    with open(path, encoding = 'utf-8') as f:\n        for line in f.readlines():\n            sp = line.strip().split()\n            num_words.append(len(sp))\n    return num_words","execution_count":8,"outputs":[]},{"metadata":{"trusted":true,"id":"wL3oUFRzK5Kb","outputId":"1c102236-b2fc-452c-cfa2-e187962ad1cf"},"cell_type":"code","source":"num_words_zh = words_count(data_dir + '/train.zh') + words_count(data_dir + '/dev.zh')\n\nplt.hist(num_words_zh, bins=200)\nplt.xlim(0, 65)\nplt.ylabel(\"number of zh words\")\nplt.xlabel(\"length of zh words\")\nplt.title(\"Distribution of zh words length\")\nplt.show()","execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true,"id":"TxWyGENzK5Kd","outputId":"01cb4a60-0eea-4037-d0fb-c8ee1e359bbe"},"cell_type":"code","source":"num_words_en = words_count(data_dir + '/train.en') + words_count(data_dir + '/dev.en')\n\nplt.hist(num_words_en, bins=200)\nplt.xlim(0, 65)\nplt.ylabel(\"number of en words\")\nplt.xlabel(\"length of en words\")\nplt.title(\"Distribution of en words length\")\nplt.show()","execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"id":"WWmubxq6K5Kf"},"cell_type":"markdown","source":"## 训练集生成"},{"metadata":{"trusted":true,"id":"96kGDOdpK5Kg"},"cell_type":"code","source":"max_len_en = 62\nmax_len_zh = 50\n# 分布式训练，全局批大小\nbatch_size = 256\nglobal_batch_size = batch_size * tpu_strategy.num_replicas_in_sync","execution_count":11,"outputs":[]},{"metadata":{"trusted":true,"id":"bWZsO51IK5Ki","outputId":"44a00139-5890-4a7e-9f4c-b215ff97099e"},"cell_type":"code","source":"\n#统计数据集大小\ndef data_count(path):    \n    with open(path,'rb') as f:\n        #统计 \\n 个数（行数）\n        count = 0\n        while True:\n            buffer = f.read(8192*1024)\n            if not buffer:\n                break\n            count += buffer.count(bytes(\"\\n\", encoding=\"utf-8\"))\n        f.close()\n        return count\n    \ncount_train = data_count(data_dir + '/train.zh')\ncount_val = data_count(data_dir + '/dev.zh')\ncount_test = data_count(data_dir + '/test.zh')\nprint(\"训练集个数：\",count_train)\nprint(\"验证集个数：\",count_val)\nprint(\"测试集个数：\",count_test)","execution_count":12,"outputs":[{"output_type":"stream","text":"训练集个数： 100000\n验证集个数： 400\n测试集个数： 400\n","name":"stdout"}]},{"metadata":{"id":"OUpVFI51CF4D","trusted":true},"cell_type":"code","source":"    # 开始和结束标记\ndef preprocess_sentence(w):\n  w = '<s> ' + w + ' </s>'\n  return w","execution_count":13,"outputs":[]},{"metadata":{"trusted":true,"id":"AMhqIuKIK5Kk"},"cell_type":"code","source":"\n# 添加开始和结束标记，并填充至maxlen\ndef pad_data(path, mxlen, zh = 1):\n    X = []\n    with open(path, encoding = 'utf-8') as f:\n        for line in f.readlines():\n            st = line.strip('\\n') #去除\\n等\n            w = preprocess_sentence(st) # 添加开始结束标记\n            sp = w.split() # 分割为列表\n            if not sp:\n                continue\n\n            x = [0] * mxlen\n            # 开始标记\n            index = 0\n            for word in sp:\n                # 控制最大长度\n                if index < mxlen:\n                    if zh == 1:\n                        if word in word2id_zh:\n                            x[index] = word2id_zh[word]\n                            index += 1\n                    else:\n                        if word in word2id_en:\n                            x[index] = word2id_en[word]\n                            index += 1\n                else:\n                    break\n            X.append(x)\n    X = np.array(X)\n    return X\n\n","execution_count":14,"outputs":[]},{"metadata":{"id":"uOIUSmkXmrB2","trusted":true},"cell_type":"code","source":"train_zh = pad_data(data_dir + '/train.zh', mxlen=max_len_zh, zh = 1)\ntrain_en = pad_data(data_dir + '/train.en', mxlen=max_len_en, zh = 0)\nval_zh = pad_data(data_dir + '/dev.zh', mxlen=max_len_zh, zh = 1)\nval_en = pad_data(data_dir + '/dev.en', mxlen=max_len_en, zh = 0)\ntest_zh = pad_data(data_dir + '/test.zh', mxlen=max_len_zh, zh = 1)\ntest_en = pad_data(data_dir + '/test.en', mxlen=max_len_en, zh = 0)","execution_count":15,"outputs":[]},{"metadata":{"id":"_hxGwo6SGN_9","outputId":"4be29296-6ad6-47f0-d0ac-58286faeb87a","trusted":true},"cell_type":"code","source":"print(test_zh[0])","execution_count":16,"outputs":[{"output_type":"stream","text":"[    1    66    24    11    32  2535 49862     4     3   659  1469     3\n  6059 14607     3  1594     4  1532   177  2071    27     4   155    28\n   229   449     4   734     5     2     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0]\n","name":"stdout"}]},{"metadata":{"id":"l5UDPtqpNZZE","outputId":"0e48511b-0f68-4028-df6a-9e9014a4792a","trusted":true},"cell_type":"code","source":"print(test_en[0])","execution_count":17,"outputs":[{"output_type":"stream","text":"[   1   82   36   13   11   11 2741 3756  111 7660   34    3 4512    9\n    3  653    6 2140   83   60 3672 3088   13    8  113    3  672 1549\n  550   17    3   62   15  186    7    2    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0]\n","name":"stdout"}]},{"metadata":{"id":"kFycgx0r4zfu"},"cell_type":"markdown","source":"zh2en"},{"metadata":{"trusted":true,"id":"yP3I0gltK5Km"},"cell_type":"code","source":"train_zh2en =  tf.data.Dataset.from_tensor_slices((train_zh, train_en)).shuffle(count_train)\n# global_batch_size\ntrain_zh2en = train_zh2en.batch(global_batch_size, drop_remainder=True)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true,"id":"aQqbIdSEK5Ko"},"cell_type":"code","source":"val_zh2en =  tf.data.Dataset.from_tensor_slices((val_zh, val_en)).batch(global_batch_size)\n# 测试集逐句\ntest_zh2en =  tf.data.Dataset.from_tensor_slices((test_zh, test_en))","execution_count":19,"outputs":[]},{"metadata":{"trusted":true,"id":"Fy_RhhR3K5Kq"},"cell_type":"code","source":"# 分布式数据集\ntrain_dataset_zh2en = tpu_strategy.experimental_distribute_dataset(train_zh2en)\nval_dataset_zh2en = tpu_strategy.experimental_distribute_dataset(val_zh2en)","execution_count":20,"outputs":[]},{"metadata":{"id":"nZciUCaf4s7F"},"cell_type":"markdown","source":"en2zh"},{"metadata":{"trusted":true,"id":"7gu7O9CO4JT7"},"cell_type":"code","source":"train_en2zh =  tf.data.Dataset.from_tensor_slices((train_en, train_zh)).shuffle(count_train)\n# global_batch_size\ntrain_en2zh = train_en2zh.batch(global_batch_size, drop_remainder=True)","execution_count":21,"outputs":[]},{"metadata":{"id":"KqSVxbd24WOz","trusted":true},"cell_type":"code","source":"val_en2zh =  tf.data.Dataset.from_tensor_slices((val_en, val_zh)).batch(global_batch_size)\n# 测试集逐句\ntest_en2zh =  tf.data.Dataset.from_tensor_slices((test_en, test_zh))","execution_count":22,"outputs":[]},{"metadata":{"id":"WMVGNTmC4I55","trusted":true},"cell_type":"code","source":"# 分布式数据集\ntrain_dataset_en2zh = tpu_strategy.experimental_distribute_dataset(train_en2zh)\nval_dataset_en2zh = tpu_strategy.experimental_distribute_dataset(val_en2zh)","execution_count":23,"outputs":[]},{"metadata":{"id":"pcHY5kATK5Kt"},"cell_type":"markdown","source":"# 构建模型"},{"metadata":{"trusted":true,"id":"0Zxobbp6K5Kt"},"cell_type":"code","source":"vocab_size_zh = len(word2id_zh)\nvocab_size_en = len(word2id_en)\n\nsteps_per_epoch = count_train//batch_size\nembedding_dim = 512\nunits = 1024 # GRU隐层神经元数","execution_count":24,"outputs":[]},{"metadata":{"id":"fyXcXjfNK5Kv"},"cell_type":"markdown","source":"## 编码器"},{"metadata":{"trusted":true,"id":"rJYh_T_0K5Kw"},"cell_type":"code","source":"class Encoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n        '''\n        :vocab_size: 词汇数\n        :embedding_dim: 词嵌入维度\n        :enc_units: 编码器中GRU层的隐节点数\n        :batch_sz: 批大小\n        '''\n        super(Encoder, self).__init__()\n        self.batch_sz = batch_sz\n        self.enc_units = enc_units\n        '''\n        embedding output:（batch size, mxlen, embedding_dim）\n        gru output: (batch size, mxlen, enc_units)\n        '''\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.gru = tf.keras.layers.GRU(self.enc_units,\n                                       return_sequences=True,\n                                       return_state=True,\n                                       recurrent_initializer='glorot_uniform') #均匀分布\n        self.dropout = tf.keras.layers.Dropout(0.2)\n        \n    def call(self, x, hidden):\n        x = self.embedding(x)\n        x = self.dropout(x)\n        \n        # GRU输出和最后一个时间步的隐层状态\n#         output, state, _ = self.gru(x, initial_state = hidden) #双向\n        output, state = self.gru(x, initial_state = hidden)\n        return output, state\n\n    def initialize_hidden_state(self):\n        # 隐节点零张量初始化\n#         return [tf.zeros((self.batch_sz, self.enc_units)) for i in range(2)] #双向\n        return [tf.zeros((self.batch_sz, self.enc_units))]","execution_count":25,"outputs":[]},{"metadata":{"id":"tZsR16n8K5Ky"},"cell_type":"markdown","source":"## 注意力机制"},{"metadata":{"trusted":true,"id":"ZkJpYpRJK5Kz"},"cell_type":"code","source":"class BahdanauAttention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(BahdanauAttention, self).__init__()\n        self.W1 = tf.keras.layers.Dense(units)\n        self.W2 = tf.keras.layers.Dense(units)\n        self.V = tf.keras.layers.Dense(1)\n\n    def call(self, query, values):\n        hidden_with_time_axis = tf.expand_dims(query, 1)\n        '''\n            :query为编码器的最后一时间步隐层状态：(batch size, enc_units)\n            :values为编码器的所有时间步的output: (batch size, mxlen, enc_units)\n\n            :拓展hidden维度, hidden_with_time_axis:（batch size，1，enc_units）\n        '''\n        \n        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n        \n        '''\n            attention_weights：注意力系数\n            values:(batch size, mxlen, enc_units) ==>（batch size，mxlen，dense_units）\n            hidden: （batch size，1，enc_units） ==> (batch size，1，dense_units)\n            Broadcasting 机制张量相加 ==>（batch size，mxlen，dense_units）\n            \n            score，最后一个状态state 和 mxlen个时间步隐层输出output的相似度: Dense(1) ==> (batch size，mxlen，1)\n        '''\n\n        # 对 mxlen所在维度softmax，即计算 mxlen个时间步的各自概率。概率大意味着相关性更高\n        # ===> (batch size, mxlen, 1)\n        attention_weights = tf.nn.softmax(score, axis=1)\n\n        # 上下文向量 = 注意力权重 * 编码器隐层输出 【对各时间步的输出加权求和】\n        context_vector = attention_weights * values\n        \n        # (batch size, mxlen, enc_units) ==> reduce_sum ==> (batch size, enc_units)\n        # 即mxlen个时间步的对应神经元值相加（mxlen个值相加），最终去除时间步维度；为了得到神经元值\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n\n        return context_vector, attention_weights","execution_count":26,"outputs":[]},{"metadata":{"id":"y5dgycskK5K1"},"cell_type":"markdown","source":"## 解码器"},{"metadata":{"trusted":true,"id":"C3bMN-y-K5K1"},"cell_type":"code","source":"class Decoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n        super(Decoder, self).__init__()\n        self.batch_sz = batch_sz\n        self.dec_units = dec_units\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.dropout = tf.keras.layers.Dropout(0.2)\n        self.gru = tf.keras.layers.GRU(\n            self.dec_units, \n            return_sequences=True,\n            return_state=True, \n            recurrent_initializer='glorot_uniform')\n        self.fc = tf.keras.layers.Dense(vocab_size)\n\n        self.attention = BahdanauAttention(self.dec_units)\n\n    def call(self, x, hidden, enc_output):\n        # 使用上次隐层state计算注意力上下文，第一次使用编码器最后output\n        context_vector, attention_weights = self.attention(hidden, enc_output)\n        \n        # x : (batch_size, 1, embedding_dim)\n        x = self.embedding(x)\n        \n        x = self.dropout(x)\n\n        # 将上一解码预测结果x（fc后） 和上下文向量结合作为本次GRU输入\n        # (batch_size, 1, embedding_dim + enc_units)\n        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n        \n        # output: (batch_size, 1, dec_units) 单个word预测\n        # state:  (batch_size, dec_units)\n        output, state = self.gru(x)\n\n        # (batch_size * 1, dec_units)\n        output = tf.reshape(output, (-1, output.shape[2])) # -1代表未知\n\n        # (batch_size, vocab_size)\n        x = self.fc(output)\n\n        return x, state, attention_weights\n","execution_count":27,"outputs":[]},{"metadata":{"id":"MVAkWiAPK5K3"},"cell_type":"markdown","source":"# 模型训练"},{"metadata":{"trusted":true,"id":"NklrwzR9K5K4"},"cell_type":"code","source":"with tpu_strategy.scope():\n\n    # 中译英\n    encoder_zh2en = Encoder(vocab_size_zh, embedding_dim, units, batch_size)\n    decoder_zh2en = Decoder(vocab_size_en, embedding_dim, units, batch_size)\n\n    # 英译中\n    encoder_en2zh = Encoder(vocab_size_en, embedding_dim, units, batch_size)\n    decoder_en2zh = Decoder(vocab_size_zh, embedding_dim, units, batch_size)\n\n","execution_count":28,"outputs":[]},{"metadata":{"id":"buWwDkXDO6UP","trusted":true},"cell_type":"code","source":"with tpu_strategy.scope():\n    # 优化器\n    optimizer = tf.keras.optimizers.Adam()\n#     train_acc = tf.metrics.SparseCategoricalAccuracy()\n\n    # 损失函数\n    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n    def loss_function(real, pred):\n        # real !=0 的返回 True\n        mask = tf.math.logical_not(tf.math.equal(real, 0)) \n        loss_ = loss_object(real, pred)\n        mask = tf.cast(mask, dtype=loss_.dtype) # cast类型转换\n        # mask为True，即real != 0时，loss不变；否则loss为0\n        loss_ *= mask\n        # return tf.reduce_mean(loss_)\n        return tf.nn.compute_average_loss(loss_, global_batch_size=global_batch_size)\n    \n    checkpoint_dir = './check'\n    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n    checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n                                     encoder=encoder_zh2en,\n                                     decoder=decoder_zh2en)","execution_count":29,"outputs":[]},{"metadata":{"id":"yDPghR_0R4yK","trusted":true},"cell_type":"code","source":"with tpu_strategy.scope():\n    class myModel():\n        # task = 1 : 中译英\n        def __init__(self, task, epochs):\n            self.task = task\n            self.EPOCHS = epochs\n            \n            if self.task == 1:\n                self.encoder = encoder_zh2en\n                self.decoder = decoder_zh2en\n                self.train_dataset = train_dataset_zh2en\n                self.val_dataset = val_dataset_zh2en\n            else:\n                self.encoder = encoder_en2zh\n                self.decoder = decoder_en2zh\n                self.train_dataset = train_dataset_en2zh\n                self.val_dataset = val_dataset_en2zh\n                \n        # 一次训练\n        def train_step(self, inp, targ, enc_hidden):\n            '''\n            inp: input sequence\n            targ: 翻译目标 sequence\n            enc_hidden: 编码器初始state\n\n            '''\n            with tf.GradientTape() as tape:\n                enc_output, enc_hidden = self.encoder(inp, enc_hidden)\n                # 编码器最后一个隐层state作为解码器第一个隐层state\n                dec_hidden = enc_hidden\n\n                # 起始字符<s>\n                # (batch_size, 1)\n                dec_input = tf.expand_dims([word2id_zh['<s>']] * batch_size, 1)\n                \n                # 教师强制，将目标词作为下一个输入\n                for t in range(1, targ.shape[1]):\n                    # x, state, attention_weights\n                    pre, dec_hidden, _ = self.decoder(dec_input, dec_hidden, enc_output)\n                    if t == 1:\n                        loss = loss_function(targ[:, t], pre)\n                    loss += loss_function(targ[:, t], pre)\n\n                    # 在训练时，每次解码器的输入并不是上次解码器的输出，而是样本目标语言对应单词\n                    dec_input = tf.expand_dims(targ[:, t], 1)\n\n                # 所有单词的平均损失值\n                # targ.shape[1] = 60或40\n                batch_loss = loss / targ.shape[1]\n\n                # 模型变量包括编码器和解码器\n                variables = self.encoder.trainable_variables + self.decoder.trainable_variables\n\n                # 计算损失函数关于自变量（模型参数）的梯度\n                gradients = tape.gradient(loss, variables)\n                # 根据梯度更新参数\n                optimizer.apply_gradients(zip(gradients, variables))\n\n            return batch_loss\n\n        # 一次验证\n        def val_step(self, inp, targ, enc_hidden):\n\n            enc_output, enc_hidden = self.encoder(inp, enc_hidden)\n            dec_hidden = enc_hidden\n            # <s> 在两个词汇表索引相同\n            dec_input = tf.expand_dims([word2id_zh['<s>']] * batch_size, 1)\n            \n            # 非教师强制\n            for t in range(1, targ.shape[1]):\n                pre, dec_hidden, _ = self.decoder(dec_input, dec_hidden, enc_output)\n\n                if t == 1:\n                      loss = loss_function(targ[:, t], pre)\n                loss += loss_function(targ[:, t], pre)\n                \n                pre_argmax = tf.argmax(pre, axis = 1)\n                dec_input = tf.expand_dims(pre_argmax, 1)\n\n            # 所有单词的平均损失值\n            batch_loss = loss/targ.shape[1]\n            return batch_loss\n\n\n        # TPU\n        # tf.distribute.ReduceOp.SUM 将各节点放缩损失相加\n        @tf.function\n        def distributed_train_step(self, inp, targ, enc_hidden):\n            assert tf.distribute.get_replica_context() is None\n            per_replica_losses = tpu_strategy.experimental_run_v2(self.train_step, args=(inp, targ, enc_hidden))\n            return tpu_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,axis=None)\n\n        @tf.function\n        def distributed_val_step(self, inp, targ, enc_hidden):\n            assert tf.distribute.get_replica_context() is None\n            per_replica_losses = tpu_strategy.experimental_run_v2(self.val_step, args=(inp, targ, enc_hidden))\n            return tpu_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,axis=None)\n        \n        # 批训练\n        def training(self):\n            print('Start Time {}\\n'.format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())))\n            for epoch in range(self.EPOCHS):\n                start = time.time()\n                print(f'Epoch {epoch + 1}/{self.EPOCHS}')\n\n                # 初始化隐藏层和损失值\n                enc_hidden = self.encoder.initialize_hidden_state()\n                total_loss = 0.0\n                num_batch = 0\n                \n                for inp, targ in self.train_dataset:\n                    batch_loss = self.distributed_train_step(inp, targ, enc_hidden)\n                    total_loss += batch_loss\n\n                    # 每50次验证集测试,并显示模型损失值\n                    if num_batch % 30 == 0:\n                        total_val_loss = 0.0\n                        \n                        for x in self.val_dataset:\n                            inp_val, targ_val = x\n                            val_loss = self.distributed_val_step(inp, targ, enc_hidden)\n                            total_val_loss += val_loss\n\n                        print('Epoch {} Batch {} Training Loss {:.4f}. Validation Loss {:.4f}.'.format(\n                                                                    epoch + 1,\n                                                                    num_batch,\n                                                                    batch_loss,\n                                                                    val_loss))\n\n                    num_batch += 1\n\n                # 每次迭代保存一次数据\n                # checkpoint.save(file_prefix=checkpoint_prefix)\n\n                # 显示每次迭代的损失值和消耗时间\n                print('Epoch {} Average  Training Loss {:.4f}. Time {} sec.\\n'.format(epoch + 1,\n                                                    total_loss / num_batch,\n                                                    time.time() - start))\n        # 测试单句\n        def test_step(self, inp, targ):\n            # 中译英\n            if self.task == 1:\n                # 目标\n                reference = [[id2word_en[x] for x in targ.numpy() if x != 0 and x != 2]]\n                # 原句\n                sentence = [id2word_zh[x] for x in inp.numpy() if x != 0]\n                # 注意力图 (result,sentence)\n                attention_plot = np.zeros((max_len_en, max_len_zh))\n                id2word = id2word_en\n            # 英译中\n            else:\n                reference = [[id2word_zh[x] for x in targ.numpy() if x != 0 and x != 2]]\n                sentence = [id2word_en[x] for x in inp.numpy() if x != 0]\n                attention_plot = np.zeros((max_len_zh, max_len_en))\n                id2word = id2word_zh\n\n            # 翻译结果\n            result = []\n\n            # No Batch\n#             hidden = [tf.zeros((1, units)) for i in range(2)]\n            hidden = [tf.zeros((1, units))]\n            inputs = tf.expand_dims(inp, 0)\n\n            enc_output, enc_hidden = self.encoder(inputs, hidden)\n            dec_hidden = enc_hidden\n            dec_input = tf.expand_dims([word2id_zh['<s>']], 0)\n            result.append('<s>')\n\n            # 非教师强制\n            for t in range(targ.shape[0]):\n                pre, dec_hidden, attention_weights = self.decoder(dec_input, dec_hidden, enc_output)\n                # 保留注意力权重用于绘制注意力图\n                attention_weights = tf.reshape(attention_weights, (-1, ))\n                attention_plot[t] = attention_weights.numpy()\n\n                # 预测值对应字符记录在result\n                pre_id = tf.argmax(pre[0]).numpy()\n                if pre_id != 2 and pre_id != 0:\n                    result.append(id2word[pre_id])\n                else:\n                    break\n\n                # 将上次预测值作为下次输入\n                dec_input = tf.expand_dims([pre_id], 0)\n\n            bleu_1 = sentence_bleu(reference, result, weights=(1, 0, 0, 0))\n            bleu_2 = sentence_bleu(reference, result, weights=(0, 1, 0, 0))\n            bleu_3 = sentence_bleu(reference, result, weights=(0, 0, 1, 0))\n            bleu_4 = sentence_bleu(reference, result, weights=(0, 0, 0, 1))\n            bleu = math.exp(bleu_1 + bleu_2 + bleu_3 + bleu_4)\n\n            return result, sentence, reference, bleu, attention_plot","execution_count":30,"outputs":[]},{"metadata":{"id":"sUyqm2CdFqiX","trusted":true},"cell_type":"code","source":"# 下载--解压--移动字体文件\n!wget \"https://www.wfonts.com/download/data/2014/06/01/simhei/simhei.zip\"\n!unzip \"simhei.zip\"\n!rm \"simhei.zip\"\n!mv SimHei.ttf /usr/share/fonts/truetype/","execution_count":31,"outputs":[{"output_type":"stream","text":"--2020-07-14 04:02:22--  https://www.wfonts.com/download/data/2014/06/01/simhei/simhei.zip\nResolving www.wfonts.com (www.wfonts.com)... 104.225.219.210\nConnecting to www.wfonts.com (www.wfonts.com)|104.225.219.210|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 10546298 (10M) [application/octetstream]\nSaving to: ‘simhei.zip’\n\nsimhei.zip          100%[===================>]  10.06M  8.89MB/s    in 1.1s    \n\n2020-07-14 04:02:24 (8.89 MB/s) - ‘simhei.zip’ saved [10546298/10546298]\n\nArchive:  simhei.zip\n  inflating: chinese.simhei.ttf      \n  inflating: SimHei.ttf              \n  inflating: sharefonts.net.txt      \n","name":"stdout"}]},{"metadata":{"trusted":true,"id":"JX3qayrQK5LA"},"cell_type":"code","source":"with tpu_strategy.scope():\n    import matplotlib.ticker as ticker\n    import matplotlib.font_manager as fm\n    path = '/usr/share/fonts/truetype/SimHei.ttf'\n    fontprop = fm.FontProperties(fname=path, size=14)\n\n    # 绘制注意力图\n    def plot_attention(attention, sentence, predicted_sentence):\n        fig = plt.figure(figsize=(len(sentence)/3,len(predicted_sentence)/3))\n        ax = fig.add_subplot(1, 1, 1)\n        ax.matshow(attention, cmap='viridis')\n\n        ax.set_xticklabels([''] + sentence, rotation=90, fontproperties=fontprop)\n        ax.set_yticklabels([''] + predicted_sentence, fontproperties=fontprop)\n\n        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n        plt.show()","execution_count":32,"outputs":[]},{"metadata":{"trusted":true,"id":"vjhuJ3hNK5K8","outputId":"bffcc62a-9bd2-4e71-f890-089afab81f73"},"cell_type":"code","source":"with tpu_strategy.scope():\n    model_z2e = myModel(0, 15) # 英译中，15 epochs\n    model_z2e.training()","execution_count":33,"outputs":[{"output_type":"stream","text":"Start Time 2020-07-14 04:02:27\n\nEpoch 1/15\nEpoch 1 Batch 0 Training Loss 5.8246. Validation Loss 5.8207.\nEpoch 1 Batch 30 Training Loss 3.9288. Validation Loss 3.9095.\nEpoch 1 Average  Training Loss 4.1225. Time 129.3183422088623 sec.\n\nEpoch 2/15\nEpoch 2 Batch 0 Training Loss 3.9019. Validation Loss 3.8848.\nEpoch 2 Batch 30 Training Loss 3.7967. Validation Loss 3.7805.\nEpoch 2 Average  Training Loss 3.8709. Time 23.03567409515381 sec.\n\nEpoch 3/15\nEpoch 3 Batch 0 Training Loss 3.7888. Validation Loss 3.7791.\nEpoch 3 Batch 30 Training Loss 3.7963. Validation Loss 3.8399.\nEpoch 3 Average  Training Loss 3.8208. Time 22.996941089630127 sec.\n\nEpoch 4/15\nEpoch 4 Batch 0 Training Loss 3.6944. Validation Loss 3.8374.\nEpoch 4 Batch 30 Training Loss 3.6464. Validation Loss 3.9632.\nEpoch 4 Average  Training Loss 3.6741. Time 22.933757066726685 sec.\n\nEpoch 5/15\nEpoch 5 Batch 0 Training Loss 3.5585. Validation Loss 3.9751.\nEpoch 5 Batch 30 Training Loss 3.5743. Validation Loss 3.9304.\nEpoch 5 Average  Training Loss 3.5786. Time 23.033700227737427 sec.\n\nEpoch 6/15\nEpoch 6 Batch 0 Training Loss 3.5713. Validation Loss 4.1735.\nEpoch 6 Batch 30 Training Loss 3.5398. Validation Loss 4.2234.\nEpoch 6 Average  Training Loss 3.5084. Time 23.01774024963379 sec.\n\nEpoch 7/15\nEpoch 7 Batch 0 Training Loss 3.4516. Validation Loss 4.2067.\nEpoch 7 Batch 30 Training Loss 3.4261. Validation Loss 4.2804.\nEpoch 7 Average  Training Loss 3.4187. Time 22.998031616210938 sec.\n\nEpoch 8/15\nEpoch 8 Batch 0 Training Loss 3.3538. Validation Loss 4.2328.\nEpoch 8 Batch 30 Training Loss 3.2919. Validation Loss 4.2283.\nEpoch 8 Average  Training Loss 3.3072. Time 23.01517653465271 sec.\n\nEpoch 9/15\nEpoch 9 Batch 0 Training Loss 3.2239. Validation Loss 4.2028.\nEpoch 9 Batch 30 Training Loss 3.2272. Validation Loss 4.2503.\nEpoch 9 Average  Training Loss 3.2089. Time 22.995965480804443 sec.\n\nEpoch 10/15\nEpoch 10 Batch 0 Training Loss 3.1113. Validation Loss 4.2143.\nEpoch 10 Batch 30 Training Loss 3.1342. Validation Loss 4.2733.\nEpoch 10 Average  Training Loss 3.1139. Time 23.00523805618286 sec.\n\nEpoch 11/15\nEpoch 11 Batch 0 Training Loss 3.0616. Validation Loss 4.2948.\nEpoch 11 Batch 30 Training Loss 2.9374. Validation Loss 4.2257.\nEpoch 11 Average  Training Loss 3.0109. Time 22.96288275718689 sec.\n\nEpoch 12/15\nEpoch 12 Batch 0 Training Loss 2.9553. Validation Loss 4.3090.\nEpoch 12 Batch 30 Training Loss 2.9069. Validation Loss 4.2680.\nEpoch 12 Average  Training Loss 2.9057. Time 23.829853296279907 sec.\n\nEpoch 13/15\nEpoch 13 Batch 0 Training Loss 2.8354. Validation Loss 4.2746.\nEpoch 13 Batch 30 Training Loss 2.8247. Validation Loss 4.2477.\nEpoch 13 Average  Training Loss 2.8078. Time 22.98498797416687 sec.\n\nEpoch 14/15\nEpoch 14 Batch 0 Training Loss 2.7025. Validation Loss 4.1479.\nEpoch 14 Batch 30 Training Loss 2.7023. Validation Loss 4.1512.\nEpoch 14 Average  Training Loss 2.7183. Time 22.965430736541748 sec.\n\nEpoch 15/15\nEpoch 15 Batch 0 Training Loss 2.6391. Validation Loss 4.1257.\nEpoch 15 Batch 30 Training Loss 2.6227. Validation Loss 4.1232.\nEpoch 15 Average  Training Loss 2.6398. Time 23.001344919204712 sec.\n\n","name":"stdout"}]},{"metadata":{"trusted":true,"id":"Nh3XjjyPK5LD"},"cell_type":"code","source":"with tpu_strategy.scope():\n    bleus = []\n    print('英译中')\n    for _, (inp, targ) in enumerate(test_en2zh.take(5)):\n        res, sent, tar, bleu, atten = model_z2e.test_step(inp, targ)\n        bleus.append(bleu)\n        print('原句：',sent)\n        print('目标：',tar[0])\n        print('结果：',res[1:])\n        print('BLEU：',bleu)\n        plot_attention(atten[:len(res[1:]), :len(sent)], sent, res[1:])\n    print(bleus)\n    # plot_attention(attentions[0][:len(results[0]), :len(sentences[0])], sentences[0], results[0])","execution_count":34,"outputs":[{"output_type":"stream","text":"中译英\n","name":"stdout"},{"output_type":"error","ename":"KeyError","evalue":"49862","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-3c95bc8c1216>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'中译英'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_zh2en\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbleu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_z2e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mbleus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbleu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'原句：'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-30-0bb0fccf1e0d>\u001b[0m in \u001b[0;36mtest_step\u001b[0;34m(self, inp, targ)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0mreference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid2word_zh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                 \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mid2word_en\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m                 \u001b[0mattention_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len_zh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len_en\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mid2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid2word_zh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-30-0bb0fccf1e0d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0mreference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid2word_zh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                 \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mid2word_en\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m                 \u001b[0mattention_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len_zh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len_en\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mid2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid2word_zh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 49862"]}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}