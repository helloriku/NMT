{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "TPU_dist.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXkBSpVSK6gi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        },
        "outputId": "26c1b83c-4af9-4cc1-bb69-2eac81172ec9"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version 2.2.0\n",
            "Running on TPU  ['10.106.67.34:8470']\n",
            "INFO:tensorflow:Initializing the TPU system: grpc://10.106.67.34:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.106.67.34:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "oZdsxDYmK5KA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d7f8b4ea-28e8-4806-e462-8b55a0d8e67f"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "print(tf.__version__)\n",
        "AUTO = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKSbVMtKMd7N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1b4b5396-e6c1-4432-8c21-efab28262408"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_AjVdL3NLQ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "36b7c3a2-6209-4390-e1d9-38683a4522be"
      },
      "source": [
        "!ls /content/drive/'My Drive'/'Colab Notebooks'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Chinese-English MT'   TPU_dist.ipynb   Untitled0.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJxA9WPuON0l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = \"/content/drive/My Drive/Colab Notebooks/Chinese-English MT\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTTtGWKXK5KM",
        "colab_type": "text"
      },
      "source": [
        "# 构建词汇表"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "OW146DbpK5KN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# 数据集已预处理好词汇表\n",
        "def build_wordlist(path):\n",
        "    word2id = {}\n",
        "    id2word = {}\n",
        "    with open(path, encoding = 'utf-8') as f:\n",
        "        for line,word in enumerate(f):\n",
        "            word2id[word.strip()] = line\n",
        "            id2word[line] = word.strip()\n",
        "    return word2id,id2word\n",
        "    "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "gcixr8sKK5KQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2id_zh, id2word_zh = build_wordlist(data_dir + '/vocab.zh')\n",
        "word2id_en, id2word_en = build_wordlist(data_dir + '/vocab.en')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "exEBgBnsK5KT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "60135e1e-cf7e-4ddd-b536-7ad6b00fe6e8"
      },
      "source": [
        "len(word2id_zh)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53712"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbEDmXfuLlim",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "10c35563-924c-42fd-c719-472de1fa83ca"
      },
      "source": [
        "len(word2id_en)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35028"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqAekFrpK5KX",
        "colab_type": "text"
      },
      "source": [
        "# 构建训练集"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgq0zzhGK5KX",
        "colab_type": "text"
      },
      "source": [
        "## 统计样本长度分布"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "iXs799gKK5KY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#统计训练样本词汇个数分布\n",
        "def words_count(path):\n",
        "    num_words = []\n",
        "    with open(path, encoding = 'utf-8') as f:\n",
        "        for line in f.readlines():\n",
        "            sp = line.strip().split()\n",
        "            num_words.append(len(sp))\n",
        "    return num_words"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wL3oUFRzK5Kb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "1c102236-b2fc-452c-cfa2-e187962ad1cf"
      },
      "source": [
        "num_words_zh = words_count(data_dir + '/train.zh') + words_count(data_dir + '/dev.zh')\n",
        "\n",
        "plt.hist(num_words_zh, bins=200)\n",
        "plt.xlim(0, 65)\n",
        "plt.ylabel(\"number of zh words\")\n",
        "plt.xlabel(\"length of zh words\")\n",
        "plt.title(\"Distribution of zh words length\")\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5hWZb3/8fdHNLU00ZjcCOhoUkbtRMND20PmEbXSrspDmmQUWWj2y90Otu0taW71srRMsx8mKmqalSaJv5QUT9sUIZGDZk6KAiHgWbFI8Pv7Y92Ty3Fm1pphnnlOn9d1rWvWutfpez88zHfWfa91L0UEZmZm3Vmv2gGYmVntc7IwM7NCThZmZlbIycLMzAo5WZiZWSEnCzMzK+RkYaVI+qmk/+qjY20t6RVJA9LyHZK+1BfHTsf7f5LG9NXxenDe70l6RtLTPdwvJG1fqbh6Q9IXJN3Ti/369N+yh+euuc+xkThZGJIWSfqbpJclvSDpXkknSPrn9yMiToiIM0oea//utomIpyJik4hY2wexT5J0VYfjHxwRV6zrsXsYx9bAKcCIiPiX/jx3M6pmUmpWThbW7hMRsSmwDXA28G3g0r4+iaT1+/qYNWJr4NmIWFHtQHqqgf9NrA85WdibRMSLETENOBIYI+mDAJIul/S9ND9I0k3pKuQ5SXdLWk/SlWS/NH+bmpn+Q1Jrah4YK+kp4PZcWf6X1HskzZL0kqQbJW2RzrWPpCX5GNuvXiSNBv4TODKd76G0/p9/daa4viPpSUkrJE2VtFla1x7HGElPpSakU7v6bCRtlvZfmY73nXT8/YEZwFYpjss72bf9M2mfXpf0hdwm+0t6LH2mF0lSJ8fYKF0BDkrLp0paI+mdafkMST/sLta07guS/lfS+ZKeBSZJepekaenznwW8J3depW1XpPXz278XRSR9UdIjkp6XdIukbXLrIl3BvqXekgZI+kH6N3lC0ont3xlJZwJ7ARemz/LCnnyO1ksR4anJJ2ARsH8n5U8BX03zlwPfS/NnAT8FNkjTXoA6OxbQCgQwFXgHsHGubP20zR3AUuCDaZtfA1eldfsAS7qKF5jUvm1u/R3Al9L8F4E2YDtgE+B64MoOsV2S4toRWA28v4vPaSpwI7Bp2vfPwNiu4uzm8z4Y+CswLC0HcBMwkCzZrgRGd7HvXcCn0/ytwF+Ag3PrPlUi1i8Aa4CTgPVT3a8Frkuf/wfTv8c9afuDgDkpPgHvBwZ3EV/+sz8sffbvT+f5DnBvbtsu6w2cADwMDAU2B37fyXfmSx3OXfpz9NTzyVcW1p2/Alt0Uv4aMBjYJiJei4i7I/1v7cakiFgVEX/rYv2VEbEgIlYB/wUcodQBvo6OAc6LiMcj4hVgInBUh6ua70bE3yLiIeAhsqTxJimWo4CJEfFyRCwCfgB8vifBSHovcAVwREQszq06OyJeiIingJnAyC4OcSfw0RT/h4AL0vJGwC7AXSVj/WtE/Dgi1gD/AD4N/Hf6N1qQYmz3GlnS2YHsj4JHImJZieqeAJyVtl8D/A8wMn910U29jwB+FBFLIuJ5sqbRMsp+jtZDThbWnSHAc52Un0v2F+Otkh6XNKHEsRb3YP2TZFcsg0pF2b2t0vHyx14f2DJXlr976VWyK5COBqWYOh5rSNlAUvPXjcB3IqLjnUZlYoAsWewD7AzMJ2v++iiwO9AWEc+WjDX/ebeQfSYd/w0AiIjbgQuBi4AVkia3N30V2Ab4UWoSeoHsu6QOcXRV7606xFP0/Sk6nq0jJwvrlKRdyP5Tv+X2yfTX6ikRsR3wSeCbkvZrX93FIYuuPIbl5rcm+2v2GWAV8PZcXAPIfrmVPe5fyX5p5Y+9BlhesF9Hz6SYOh5raZmdU3/Bz4GZETG5h+fOuxd4H/Ap4M6IeDjFcQhZIikba/5zW0n2mXT8N3hj44gLIuLDwAjgvcC3SsS6GPhKRAzMTRtHxL0l9l1G1gTVbliH9R4uu585WdibSHqnpI+TtWFfFRHzO9nm45K2T52HLwJrgdfT6uVk/QM9daykEZLeDpwO/CqyW2v/DGwk6VBJG5C1e2+Y22850Krcbb4dXAP8H0nbStqErCnkF6lZpLQUy3XAmZI2TU0p3wSu6n7PfzqTrD/g5J6ct5M4XiXrPxjPG8nhXrImnzt7E2va/nqyju63SxoB/PM5FUm7SNotff6rgL/zxr93d34KTJT0gXSczSR9tmRVrwNOljRE0kCyu/Pyevs9s15ysrB2v5X0Mtlfg6cC5wHHd7HtcLIOx1eAPwA/iYiZad1ZwHdS08O/9+D8V5J1oj8NbAR8HbK7s4CvAT8j+8t4FZC/O+qX6eezkv7YyXGnpGPfBTxB9ovupB7ElXdSOv/jZFdcP0/HL+Nosqai53N3RB3TyzjuJGtmmpVb3pSsjr2N9USyJpunyf4dLsuteyfZTQDPkzVPPUvWFNmtiLgBOAe4VtJLwAKyzv0yLiHrwJ8HPAjcTHb10/5szo+Az6S7rC4oeUxbB+13sJiZ1SxJBwM/jYhtCje2ivCVhZnVHEkbSzokPVcxBDgNuKHacTUzX1mYWc1JfVd3kt2u+zdgOnByRLxU1cCamJOFmZkVcjOUmZkVasgBxAYNGhStra3VDsPMrK7MmTPnmYho6WxdQyaL1tZWZs+eXe0wzMzqiqQnu1rnZigzMyvkZGFmZoWcLMzMrJCThZmZFXKyMDOzQk4WZmZWyMnCzMwKOVmYmVkhJwszMyvkZGFV1zpherVDMLMCThZmZlbIycLMzAo5WZiZWSEnC+tXrROmu4/CrA5VLFlI2kjSLEkPSVoo6bup/HJJT0iam6aRqVySLpDUJmmepJ1zxxoj6bE0jalUzNZ3nBTMGkslryxWA/tGxI7ASGC0pN3Tum9FxMg0zU1lBwPD0zQOuBhA0hZkL2vfDdgVOE3S5hWM27pRzQTgBGRWPRVLFpF5JS1ukKbuXvh9GDA17XcfMFDSYOAgYEZEPBcRzwMzgNGVitvMzN6qon0WkgZImgusIPuFf39adWZqajpf0oapbAiwOLf7klTWVXnHc42TNFvS7JUrV/Z5XczMmllFk0VErI2IkcBQYFdJHwQmAjsAuwBbAN/uo3NNjohRETGqpaXTV8haBbl5yKyx9cvdUBHxAjATGB0Ry1JT02rgMrJ+CIClwLDcbkNTWVflZk5SZv2kkndDtUgamOY3Bg4A/pT6IZAk4HBgQdplGnBcuitqd+DFiFgG3AIcKGnz1LF9YCozM7N+sn4Fjz0YuELSALKkdF1E3CTpdkktgIC5wAlp+5uBQ4A24FXgeICIeE7SGcADabvTI+K5CsZtZmYdVCxZRMQ8YKdOyvftYvsAxnexbgowpU8DtJrV3rS06OxD1+kY67K/mb2Zn+C2puBnNMzWjZOFdcm/YM2snZOFmZkVcrIwM7NCThZmZlbIycLMzAo5WZiZWSEnCzMzK+RkYWZmhZwszM9TmFkhJwszMyvkZGFmZoWcLKypuQnOrBwnCzMzK+RkYWZmhZwszMyskJOFmZkVcrIw68Ad3mZv5WRhZmaFnCzMzKxQxZKFpI0kzZL0kKSFkr6byreVdL+kNkm/kPS2VL5hWm5L61tzx5qYyh+VdFClYm4WbmYxs56q5JXFamDfiNgRGAmMlrQ7cA5wfkRsDzwPjE3bjwWeT+Xnp+2QNAI4CvgAMBr4iaQBFYzbzMw6qFiyiMwraXGDNAWwL/CrVH4FcHiaPywtk9bvJ0mp/NqIWB0RTwBtwK6VitvMzN6qon0WkgZImgusAGYAfwFeiIg1aZMlwJA0PwRYDJDWvwi8K1/eyT75c42TNFvS7JUrV1aiOmZmTauiySIi1kbESGAo2dXADhU81+SIGBURo1paWip1GjOzptQvd0NFxAvATOAjwEBJ66dVQ4GlaX4pMAwgrd8MeDZf3sk+ZmbWDyp5N1SLpIFpfmPgAOARsqTxmbTZGODGND8tLZPW3x4RkcqPSndLbQsMB2ZVKm6zrvguMmtm6xdv0muDgSvSnUvrAddFxE2SHgaulfQ94EHg0rT9pcCVktqA58jugCIiFkq6DngYWAOMj4i1FYy7YbT/clt09qFVjsTM6l3FkkVEzAN26qT8cTq5myki/g58totjnQmc2dcxmplZOX6C28zMCjlZmJlZIScLMzMr5GRhZmaFnCzMzKyQk4WZmRVysjAzs0KFyULSHpLekeaPlXSepG0qH5pZbWudMN1PdVvTKHNlcTHwqqQdgVPIRo6dWtGozMysppRJFmvSGE2HARdGxEXAppUNy8zMakmZ4T5eljQROBbYW9J6ZC8yMrNOeEwua0RlriyOJHtF6tiIeJpsiPBzKxqVmZnVlMJkERFPR8R5EXF3Wn4qItxnUYPc2WpmldJlM5Skl8nemd2piHhnRSIyM7Oa02WyiIhNASSdASwDrgQEHEP2rgozM2sSZfosPhkRP4mIlyPipYi4mOzOKDMzaxJlksUqScdIGiBpPUnHAKsqHZiZmdWOMsnic8ARwPI0fTaVmZlZk+j2OYv0/uwTI8LNTmZmTazbK4uIWAvs2ZsDSxomaaakhyUtlHRyKp8kaamkuWk6JLfPREltkh6VdFCufHQqa5M0oTfxmJlZ75V5gvtBSdOAX5Lrq4iI6wv2WwOcEhF/lLQpMEfSjLTu/Ij4fn5jSSOAo4APAFsBv5f03rT6IuAAYAnwgKRpEfFwidjNakLrhOl+otvqWplksRHwLLBvriyAbpNFRCwju+WWiHhZ0iPAkG52OQy4NiJWA09IagN2TevaIuJxAEnXpm2dLMzM+klhsoiI49f1JJJagZ2A+4E9gBMlHQfMJrv6eJ4skdyX220JbySXxR3Kd+vkHOOAcQBbb731uoZsZmY5Zd5nMVTSDZJWpOnXkoaWPYGkTYBfA9+IiJfIhjx/DzCS7MrjB72M/U0iYnJEjIqIUS0tLX1xSLOK8bswrN6UuXX2MmAaWT/CVsBvU1khSRuQJYqr2/s4ImJ5RKyNiNeBS3ijqWkpMCy3+9BU1lW5mZn1kzLJoiUiLouINWm6HCj8012SgEuBRyLivFx5fqiQTwEL0vw04ChJG0raFhgOzAIeAIZL2lbS28g6waeViNvMzPpImQ7uZyUdC1yTlo8m6/AusgfweWC+pLmp7D+BoyWNJOskXwR8BSAiFkq6jqzjeg0wPt26i6QTgVuAAcCUiFhY4vxmZtZHyiSLLwI/Bs4n+wV/L1DY6R0R95ANPNjRzd3scyZwZiflN3e3n5mZVVaZZLE8Ij5Z8UjMzKxmlUkWCyQtB+5O0z0R8WJlwzIzs1pS5k1525P1U8wHDgUeyvVBmJlZEyi8skjPVOwB7AXsCCwE7qlwXGZmVkPKNEM9RXb76v9ExAkVjsfMzGpQmecsdgKmAp+T9AdJUyWNrXBcZk3JT3ZbrSozNtRDkv4C/IWsKepY4KNkD9yZmVkTKNNnMRvYkOz5iruBvSPiyUoHZmZmtaNMn8XBEbGy4pFYj7Q3VfgdCWbWH8rcOutEYWbW5Mp0cJuZWZNzsjAzs0Jl+iyQ9G9Aa377iJhaoZjMzKzGlLkb6kqyN9vNBdam4iB79sLMzJpAmSuLUcCIiIhKB2NmZrWpTJ/FAuBfKh2ImXXOT3RbLejyykLSb8mamzYFHpY0C1jdvt7vuDAzax7dNUN9v9+iMDOzmtZlM1RE3BkRdwJ7A4+3L6ey9/VbhGZmVnVl+ixOAn4n6WO5ssKhyiUNkzRT0sOSFko6OZVvIWmGpMfSz81TuSRdIKlN0jxJO+eONSZt/5ikMT2so5mZraMyyWIpcDBwtqRvpTKV2G8NcEpEjAB2B8ZLGgFMAG6LiOHAbWmZdI7haRoHXAxZcgFOA3YDdgVOa08wZmbWP0o9wR0RT5ENSz5C0i+BjUvssywi/pjmXwYeAYYAhwFXpM2uAA5P84cBUyNzHzBQ0mDgIGBGRDwXEc8DM4DRZStoZmbrrkyymA0QEX+PiOOBO4C39eQkklrJXqJ0P7BlRCxLq54GtkzzQ4DFud2WpLKuyjueY5yk2ZJmr1zpsQ/NzPpSmVFnv9xh+aKI2K7sCSRtAvwa+EZEvNThWEF2e+46i4jJETEqIka1tLT0xSHNzCyp6ECCkjYgSxRXR8T1qXh5al4i/VyRypcCw3K7D01lXZWbNTU/rGf9qWLJQpLIXr36SEScl1s1DWi/o2kMcGOu/Lh0V9TuwIupueoW4EBJm6eO7QNTmZmZ9ZMuk0UaQJD2W157YQ/g88C+kuam6RDgbOAASY8B+6dlgJuBx4E24BLgawAR8RxwBvBAmk5PZU2hdcJ0/wVpZlXX3RPcH5a0FfBFSVPpcLts0S/siLin4z45+3WyfQDjuzjWFGBKd+czM7PK6S5Z/JTsOYjtgDm8+Rd/pHIzM2sC3Q33cUFEvB+YEhHbRcS2ucmJwsysiRS+zyIivippR2CvVHRXRMyrbFhmZlZLCu+GkvR14Grg3Wm6WtJJlQ7MzHrGN0JYJZV5U96XgN0iYhWApHOAPwA/rmRgZmZWO8o8ZyHeePc2ab7MQIJmZtYgylxZXAbcL+mGtHw42cN2ZmbWJMp0cJ8n6Q5gz1R0fEQ8WNGozMysppS5siANNf7HCsdiZmY1qqIDCZqZWWNwsjAzs0LdJgtJAyTN7K9gzMysNnWbLCJiLfC6pM36KR4zM6tBZTq4XwHmS5oBrGovjIivVyyqJtY6YTqLzj602mGYmb1JmWRxfZrMzKxJlXnO4gpJGwNbR8Sj/RCTmfUhX61aXygzkOAngLnA79LySEnTKh2YmZnVjjK3zk4CdgVeAIiIufjFR2ZmTaVMsngtIl7sUPZ6JYIxM7PaVKaDe6GkzwEDJA0Hvg7cW9mwzMyslpS5sjgJ+ACwGrgGeAn4RtFOkqZIWiFpQa5skqSlkuam6ZDcuomS2iQ9KumgXPnoVNYmaUJPKmdmZn2jzN1QrwKnppceRUS8XPLYlwMXAlM7lJ8fEd/PF0gaARxFlpS2An4v6b1p9UXAAcAS4AFJ0yLi4ZIxmJlZHyhzN9QukuYD88gezntI0oeL9ouIu4DnSsZxGHBtRKyOiCeANrJO9V2Btoh4PCL+AVybtjUzs35UphnqUuBrEdEaEa3AeLIXIvXWiZLmpWaqzVPZEGBxbpslqayr8reQNE7SbEmzV65cuQ7hmZlZR2WSxdqIuLt9ISLuAdb08nwXA+8BRgLLgB/08jhvERGTI2JURIxqaWnpq8OamRnd9FlI2jnN3inp/5J1bgdwJHBHb04WEctzx78EuCktLgWG5TYdmsroptzMzPpJdx3cHf/qPy03H705maTBEbEsLX4KaL9Tahrwc0nnkXVwDwdmAQKGS9qWLEkcBXyuN+c2s0zrhOkAHgLEeqTLZBERH1uXA0u6BtgHGCRpCVmy2UfSSLJkswj4SjrXQknXAQ+TNXGNT8OjI+lE4BZgADAlIhauS1xmZtZzhbfOShoIHAe05rcvGqI8Io7upPjSbrY/Ezizk/KbgZuL4jQzs8op8wT3zcB9wHw8zIdZw3LzlHWnTLLYKCK+WfFIzMysZpW5dfZKSV+WNFjSFu1TxSMzM7OaUebK4h/AucCpvHEXVOBhys3MmkaZZHEKsH1EPFPpYMzMrDaVaYZqA16tdCDNpr0z0cysHpS5slgFzJU0k2yYcqD41lkzM2scZZLFb9JkZmZNqsz7LK7oj0DMrPa0Tpju5y4MKPcE9xN0MhZURPhuKDOzJlGmGWpUbn4j4LOAn7MwM2sihXdDRcSzuWlpRPwQ8HWpmVkTKdMMtXNucT2yK40yVyRmZtYgyvzSz7/XYg3Z0OJHVCQaMzOrSWXuhlqn91qYmVn9K9MMtSHwad76PovTKxeWmZnVkjLNUDcCLwJzyD3BbWZmzaNMshgaEaMrHomZmdWsMgMJ3ivpXyseiZmZ1awyyWJPYI6kRyXNkzRf0ryinSRNkbRC0oJc2RaSZkh6LP3cPJVL0gWS2tI5ds7tMyZt/5ikMb2ppJmZrZsyyeJgYDhwIPAJ4OPpZ5HLgY7NVxOA2yJiOHBbWs6fYzgwDrgYsuQCnAbsBuwKnNaeYMys/7VOmO7h9ZtUmVtnn+zNgSPiLkmtHYoPA/ZJ81cAdwDfTuVTIyKA+yQNlDQ4bTsjIp4DkDSDLAFd05uYzMysd8pcWfSlLSNiWZp/GtgyzQ8BFue2W5LKuip/C0njJM2WNHvlypV9G7WZWZPr72TxT+kq4i2j2a7D8SZHxKiIGNXS0tJXhzWzEtw81fj6O1ksT81LpJ8rUvlSYFhuu6GprKtyMzPrR/2dLKYB7Xc0jSF74K+9/Lh0V9TuwIupueoW4EBJm6eO7QNTmZmZ9aOKjR4r6RqyDupBkpaQ3dV0NnCdpLHAk7wxIOHNwCFAG/AqcDxARDwn6QzggbTd6e2d3WZm1n8qliwi4uguVu3XybYBjO/iOFOAKX0YWr9rb8v16ynNrF5VrYPbzMzqh5OFmZkVcrIwM7NCThZmZlbIycLMzAo5WZiZWSEnCzOrCA//0VicLMzMrJCThZmZFXKyMDOzQk4WZtav3JdRn5wszMyskJOFmZkVcrIwM7NCThZmZlbIyaIP+T3EZtaonCzMzKyQk4WZVZWvyOuDk4WZmRWqSrKQtEjSfElzJc1OZVtImiHpsfRz81QuSRdIapM0T9LO1YjZzKyZVfPK4mMRMTIiRqXlCcBtETEcuC0tAxwMDE/TOODifo/UzKzJ1VIz1GHAFWn+CuDwXPnUyNwHDJQ0uBoBmpk1q2oliwBulTRH0rhUtmVELEvzTwNbpvkhwOLcvktS2ZtIGidptqTZK1eurFTcZtZP3PFdW9av0nn3jIilkt4NzJD0p/zKiAhJ0ZMDRsRkYDLAqFGjerSvmZl1rypXFhGxNP1cAdwA7Aosb29eSj9XpM2XAsNyuw9NZWbWZHylUT39niwkvUPSpu3zwIHAAmAaMCZtNga4Mc1PA45Ld0XtDryYa64yM7N+UI1mqC2BGyS1n//nEfE7SQ8A10kaCzwJHJG2vxk4BGgDXgWO7/+QzcyaW78ni4h4HNixk/Jngf06KQ9gfD+EZmZmXailW2frjttPzaxZOFmYmVkhJwszMyvkZGFmZoWcLMys7rn/sPKcLMzMrJCThZk1HI8r1fecLMzMrJCThZk1BV9prBsnCzMzK+RkYWZNzVcc5ThZmJlZIScLMzMr5GRRgi9TzZqHb7vtnJOFmVkJzZ5EnCzMzKyQk4WZWS8105WGk4WZmRVysjAz62Mdrzh60t9Rq1crThYdNHsnlplZZ+omWUgaLelRSW2SJlQ7HjOzdVXmj9Na+eO1LpKFpAHARcDBwAjgaEkjqhuVmVnzqItkAewKtEXE4xHxD+Ba4LC+OHCtZG0zs1qmiKh2DIUkfQYYHRFfSsufB3aLiBNz24wDxqXFDwIL+j3QyhkEPFPtIPpII9UFGqs+jVQXaKz69FddtomIls5WrN8PJ+8XETEZmAwgaXZEjKpySH2mkerTSHWBxqpPI9UFGqs+tVCXemmGWgoMyy0PTWVmZtYP6iVZPAAMl7StpLcBRwHTqhyTmVnTqItmqIhYI+lE4BZgADAlIhZ2s8vk/oms3zRSfRqpLtBY9WmkukBj1afqdamLDm4zM6uuemmGMjOzKnKyMDOzQg2XLOp9WBBJUyStkLQgV7aFpBmSHks/N69mjGVJGiZppqSHJS2UdHIqr7v6SNpI0ixJD6W6fDeVbyvp/vR9+0W6AaNuSBog6UFJN6XluqyPpEWS5kuaK2l2Kqu771k7SQMl/UrSnyQ9Iukj1a5PQyWLBhkW5HJgdIeyCcBtETEcuC0t14M1wCkRMQLYHRif/j3qsT6rgX0jYkdgJDBa0u7AOcD5EbE98Dwwtoox9sbJwCO55Xquz8ciYmTueYR6/J61+xHwu4jYAdiR7N+ouvWJiIaZgI8At+SWJwITqx1XL+rRCizILT8KDE7zg4FHqx1jL+t1I3BAvdcHeDvwR2A3sqdq10/lb/r+1fpE9rzSbcC+wE2A6rU+wCJgUIeyuvyeAZsBT5BuQKqV+jTUlQUwBFicW16SyurdlhGxLM0/DWxZzWB6Q1IrsBNwP3Van9RkMxdYAcwA/gK8EBFr0ib19n37IfAfwOtp+V3Ub30CuFXSnDT0D9Tp9wzYFlgJXJaaCH8m6R1UuT6NliwaXmR/VtTV/c6SNgF+DXwjIl7Kr6un+kTE2ogYSfYX+a7ADlUOqdckfRxYERFzqh1LH9kzInYma4IeL2nv/Mp6+p6RPf+2M3BxROwErKJDk1M16tNoyaJRhwVZLmkwQPq5osrxlCZpA7JEcXVEXJ+K67Y+ABHxAjCTrJlmoKT2h1vr6fu2B/BJSYvIRnHel6ydvC7rExFL088VwA1kybxev2dLgCURcX9a/hVZ8qhqfRotWTTqsCDTgDFpfgxZ23/NkyTgUuCRiDgvt6ru6iOpRdLANL8xWd/LI2RJ4zNps7qoC0BETIyIoRHRSvb/5PaIOIY6rI+kd0jatH0eOJBs1Om6+54BRMTTwGJJ70tF+wEPU+36VLszpwKdQ4cAfyZrTz612vH0Iv5rgGXAa2R/YYwla0u+DXgM+D2wRbXjLFmXPckulecBc9N0SD3WB/gQ8GCqywLgv1P5dsAsoA34JbBhtWPtRd32AW6q1/qkmB9K08L2//f1+D3L1WkkMDt9334DbF7t+ni4DzMzK9RozVBmZlYBThZmZlbIycLMzAo5WZiZWSEnCzMzK+RkYXVN0isVOOZISYfklidJ+vd1ON5n08ihM0ts25ofcbi/pdFbB1Xr/Fa7nCzM3mok2fMgfWUs8OWI+FgfHnOd5Z7UNivkZGENQ9K3JD0gaV7ufROt6a/6S9J7KG5NT2AjaZe07VxJ50pakJ78Px04MpUfmQ4/QtIdkh6X9PUuzn90eqfCAknnpLL/Jns48VJJ53bY/vR0jrmSlkq6LK0a0Fm8uf0GSHpCmYGS1raPhSTpLknD07sPfpPqd5+kD6X1kyRdKel/gSslvSudY6Gkn5GNPNv+VPR0Ze/vWJD7HJUUgdoAAAKpSURBVKxZVftJRU+e1mUCXkk/DyR7qb3I/gi6CdibbLj3NcDItN11wLFpfgHwkTR/NmlYeOALwIW5c0wC7gU2BAYBzwIbdIhjK+ApoIVsILjbgcPTujuAUd3UYSAwH/hwd/F22Od3wAeAj5MNc3Nqiu+JtP7HwGlpfl9gbq4uc4CN0/IFvPE0+qFkT9wPAj4NXJI732bV/rf2VN3JVxbWKA5M04Nk75rYARie1j0REXPT/BygNY3ztGlE/CGV/7zg+NMjYnVEPEM2gFvH4aF3Ae6IiJWRDfF9NVmy6lYaP+sq4Lx4YwTYt8Tbya53p+PvDZxFdvWyC1niIC1fCRARtwPvkvTOtG5aRPwtze+dzk9ETCd74RFkyesASedI2isiXiyqizU2JwtrFALOiuxNaSMjYvuIuDStW53bbi3ZX/491RfH6MwkshFGL8uVlTnXXcBeZKOr3kx2dbIPWRIpsqpog4j4M9lIp/OB76XmNGtiThbWKG4BvpjenYGkIZLe3dXGkQ0z/rKk3VLRUbnVLwOb9vD8s4CPShqUXu97NHBndztI+gSwP9BpH0iJ8/0b8HpE/J1skMavkCURyJLGMek8+wDPRId3iSR3AZ9L2x1MNmAdkrYCXo2Iq4BzyRKHNTHfDWENISJulfR+4A9Zyw6vAMeS/WXelbHAJZJeJ/vF3t7UMhOYoOyteGeVPP8ySRPSviJrtioaQvqbZG+im5VingZMKXm+1ZIWA/elorvJEtT8tDwJmCJpHvAqbwxt3dF3gWskLSTrl3kqlf8rcG76bF4DvlomLmtcHnXWmpakTSLilTQ/gez9xidXOSyzmuQrC2tmh0qaSPb/4Emyu6DMrBO+sjAzs0Lu4DYzs0JOFmZmVsjJwszMCjlZmJlZIScLMzMr9P8BGeVTidpmuaUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "TxWyGENzK5Kd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "01cb4a60-0eea-4037-d0fb-c8ee1e359bbe"
      },
      "source": [
        "num_words_en = words_count(data_dir + '/train.en') + words_count(data_dir + '/dev.en')\n",
        "\n",
        "plt.hist(num_words_en, bins=200)\n",
        "plt.xlim(0, 65)\n",
        "plt.ylabel(\"number of en words\")\n",
        "plt.xlabel(\"length of en words\")\n",
        "plt.title(\"Distribution of en words length\")\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWmubxq6K5Kf",
        "colab_type": "text"
      },
      "source": [
        "## 训练集生成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "96kGDOdpK5Kg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len_en = 60\n",
        "max_len_zh = 50\n",
        "# 分布式训练，全局批大小\n",
        "batch_size = 128\n",
        "global_batch_size = batch_size * tpu_strategy.num_replicas_in_sync"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bWZsO51IK5Ki",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "44a00139-5890-4a7e-9f4c-b215ff97099e"
      },
      "source": [
        "\n",
        "#统计数据集大小\n",
        "def data_count(path):    \n",
        "    with open(path,'rb') as f:\n",
        "        #统计 \\n 个数（行数）\n",
        "        count = 0\n",
        "        while True:\n",
        "            buffer = f.read(8192*1024)\n",
        "            if not buffer:\n",
        "                break\n",
        "            count += buffer.count(bytes(\"\\n\", encoding=\"utf-8\"))\n",
        "        f.close()\n",
        "        return count\n",
        "    \n",
        "count_train = data_count(data_dir + '/train.zh')\n",
        "count_val = data_count(data_dir + '/dev.zh')\n",
        "count_test = data_count(data_dir + '/test.zh')\n",
        "print(\"训练集个数：\",count_train)\n",
        "print(\"验证集个数：\",count_val)\n",
        "print(\"测试集个数：\",count_test)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "训练集个数： 100000\n",
            "验证集个数： 400\n",
            "测试集个数： 400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUpVFI51CF4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # 开始和结束标记\n",
        "def preprocess_sentence(w):\n",
        "  w = '<s> ' + w + ' </s>'\n",
        "  return w"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "AMhqIuKIK5Kk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# 添加开始和结束标记，并填充至maxlen\n",
        "def pad_data(path, mxlen, zh = 1):\n",
        "    X = []\n",
        "    with open(path, encoding = 'utf-8') as f:\n",
        "        for line in f.readlines():\n",
        "            st = line.strip('\\n') #去除\\n等\n",
        "            w = preprocess_sentence(st) # 添加开始结束标记\n",
        "            sp = w.split() # 分割为列表\n",
        "            if not sp:\n",
        "                continue\n",
        "\n",
        "            x = [0] * mxlen\n",
        "            # 开始标记\n",
        "            index = 0\n",
        "            for word in sp:\n",
        "                # 控制最大长度\n",
        "                if index < mxlen:\n",
        "                    if zh == 1:\n",
        "                        if word in word2id_zh:\n",
        "                            x[index] = word2id_zh[word]\n",
        "                            index += 1\n",
        "                    else:\n",
        "                        if word in word2id_en:\n",
        "                            x[index] = word2id_en[word]\n",
        "                            index += 1\n",
        "                else:\n",
        "                    break\n",
        "            X.append(x)\n",
        "    X = np.array(X)\n",
        "    return X\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOIUSmkXmrB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_zh = pad_data(data_dir + '/train.zh', mxlen=max_len_zh, zh = 1)\n",
        "train_en = pad_data(data_dir + '/train.en', mxlen=max_len_en, zh = 0)\n",
        "val_zh = pad_data(data_dir + '/dev.zh', mxlen=max_len_zh, zh = 1)\n",
        "val_en = pad_data(data_dir + '/dev.en', mxlen=max_len_en, zh = 0)\n",
        "test_zh = pad_data(data_dir + '/test.zh', mxlen=max_len_zh, zh = 1)\n",
        "test_en = pad_data(data_dir + '/test.en', mxlen=max_len_en, zh = 0)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hxGwo6SGN_9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "4be29296-6ad6-47f0-d0ac-58286faeb87a"
      },
      "source": [
        "print(test_zh[0])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[    1    66    24    11    32  2535 49862     4     3   659  1469     3\n",
            "  6059 14607     3  1594     4  1532   177  2071    27     4   155    28\n",
            "   229   449     4   734     5     2     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5UDPtqpNZZE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "0e48511b-0f68-4028-df6a-9e9014a4792a"
      },
      "source": [
        "print(test_en[0])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   1   82   36   13   11   11 2741 3756  111 7660   34    3 4512    9\n",
            "    3  653    6 2140   83   60 3672 3088   13    8  113    3  672 1549\n",
            "  550   17    3   62   15  186    7    2    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFycgx0r4zfu",
        "colab_type": "text"
      },
      "source": [
        "zh2en"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yP3I0gltK5Km",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_zh2en =  tf.data.Dataset.from_tensor_slices((train_zh, train_en)).shuffle(count_train)\n",
        "# global_batch_size\n",
        "train_zh2en = train_zh2en.batch(global_batch_size, drop_remainder=True)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "aQqbIdSEK5Ko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_zh2en =  tf.data.Dataset.from_tensor_slices((val_zh, val_en)).batch(global_batch_size)\n",
        "# 测试集逐句\n",
        "test_zh2en =  tf.data.Dataset.from_tensor_slices((test_zh, test_en))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Fy_RhhR3K5Kq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 分布式数据集\n",
        "train_dataset_zh2en = tpu_strategy.experimental_distribute_dataset(train_zh2en)\n",
        "val_dataset_zh2en = tpu_strategy.experimental_distribute_dataset(val_zh2en)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZciUCaf4s7F",
        "colab_type": "text"
      },
      "source": [
        "en2zh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab_type": "code",
        "id": "7gu7O9CO4JT7",
        "colab": {}
      },
      "source": [
        "train_en2zh =  tf.data.Dataset.from_tensor_slices((train_en, train_zh)).shuffle(count_train)\n",
        "# global_batch_size\n",
        "train_en2zh = train_en2zh.batch(global_batch_size, drop_remainder=True)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqSVxbd24WOz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_en2zh =  tf.data.Dataset.from_tensor_slices((val_en, val_zh)).batch(global_batch_size)\n",
        "# 测试集逐句\n",
        "test_en2zh =  tf.data.Dataset.from_tensor_slices((test_en, test_zh))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMVGNTmC4I55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 分布式数据集\n",
        "train_dataset_en2zh = tpu_strategy.experimental_distribute_dataset(train_en2zh)\n",
        "val_dataset_en2zh = tpu_strategy.experimental_distribute_dataset(val_en2zh)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcHY5kATK5Kt",
        "colab_type": "text"
      },
      "source": [
        "# 构建模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "0Zxobbp6K5Kt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size_zh = len(word2id_zh)\n",
        "vocab_size_en = len(word2id_en)\n",
        "\n",
        "steps_per_epoch = count_train//batch_size\n",
        "embedding_dim = 256\n",
        "units = 1024 # GRU隐层神经元数"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyXcXjfNK5Kv",
        "colab_type": "text"
      },
      "source": [
        "## 编码器"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rJYh_T_0K5Kw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        '''\n",
        "        :vocab_size: 词汇数\n",
        "        :embedding_dim: 词嵌入维度\n",
        "        :enc_units: 编码器中GRU层的隐节点数\n",
        "        :batch_sz: 批大小\n",
        "        '''\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        '''\n",
        "        embedding output:（batch size, mxlen, embedding_dim）\n",
        "        gru output: (batch size, mxlen, enc_units)\n",
        "        '''\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform') #均匀分布\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        # GRU输出和最后一个时间步的隐层状态\n",
        "        output, state = self.gru(x, initial_state = hidden) #初始状态张量列表\n",
        "        return output, state\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        # 隐节点零张量初始化\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZsR16n8K5Ky",
        "colab_type": "text"
      },
      "source": [
        "## 注意力机制"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ZkJpYpRJK5Kz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "        '''\n",
        "            :query为编码器的最后一时间步隐层状态：(batch size, enc_units)\n",
        "            :values为编码器的所有时间步的output: (batch size, mxlen, enc_units)\n",
        "\n",
        "            :拓展hidden维度, hidden_with_time_axis:（batch size，1，enc_units）\n",
        "        '''\n",
        "        \n",
        "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "        \n",
        "        '''\n",
        "            attention_weights：注意力系数\n",
        "            values:(batch size, mxlen, enc_units) ==>（batch size，mxlen，dense_units）\n",
        "            hidden: （batch size，1，enc_units） ==> (batch size，1，dense_units)\n",
        "            Broadcasting 机制张量相加 ==>（batch size，mxlen，dense_units）\n",
        "            \n",
        "            score，最后一个状态state 和 mxlen个时间步隐层输出output的相似度: Dense(1) ==> (batch size，mxlen，1)\n",
        "        '''\n",
        "\n",
        "        # 对 mxlen所在维度softmax，即计算 mxlen个时间步的各自概率。概率大意味着相关性更高\n",
        "        # ===> (batch size, mxlen, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # 上下文向量 = 注意力权重 * 编码器隐层输出 【对各时间步的输出加权求和】\n",
        "        context_vector = attention_weights * values\n",
        "        \n",
        "        # (batch size, mxlen, enc_units) ==> reduce_sum ==> (batch size, enc_units)\n",
        "        # 即mxlen个时间步的对应神经元值相加（mxlen个值相加），最终去除时间步维度；为了得到神经元值\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5dgycskK5K1",
        "colab_type": "text"
      },
      "source": [
        "## 解码器"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "C3bMN-y-K5K1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(\n",
        "            self.dec_units, \n",
        "            return_sequences=True,\n",
        "            return_state=True, \n",
        "            recurrent_initializer='glorot_uniform')\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # 使用上次隐层state计算注意力上下文，第一次使用编码器最后output\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "        \n",
        "        # x : (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # 将上一解码预测结果x（fc后） 和上下文向量结合作为本次GRU输入\n",
        "        # (batch_size, 1, embedding_dim + enc_units)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        \n",
        "        # output: (batch_size, 1, dec_units) 单个word预测\n",
        "        # state:  (batch_size, dec_units)\n",
        "        output, state = self.gru(x)\n",
        "\n",
        "        # (batch_size * 1, dec_units)\n",
        "        output = tf.reshape(output, (-1, output.shape[2])) # -1代表未知\n",
        "\n",
        "        # (batch_size, vocab_size)\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, state, attention_weights\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVAkWiAPK5K3",
        "colab_type": "text"
      },
      "source": [
        "# 模型训练"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "NklrwzR9K5K4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tpu_strategy.scope():\n",
        "\n",
        "    # 中译英\n",
        "    encoder_zh2en = Encoder(vocab_size_zh, embedding_dim, units, batch_size)\n",
        "    decoder_zh2en = Decoder(vocab_size_en, embedding_dim, units, batch_size)\n",
        "\n",
        "    # 英译中\n",
        "    encoder_en2zh = Encoder(vocab_size_en, embedding_dim, units, batch_size)\n",
        "    decoder_en2zh = Decoder(vocab_size_zh, embedding_dim, units, batch_size)\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buWwDkXDO6UP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tpu_strategy.scope():\n",
        "    # 优化器\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "    # 损失函数\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "    def loss_function(real, pred):\n",
        "        # real !=0 的返回 True\n",
        "        mask = tf.math.logical_not(tf.math.equal(real, 0)) \n",
        "        loss_ = loss_object(real, pred)\n",
        "        mask = tf.cast(mask, dtype=loss_.dtype) # cast类型转换\n",
        "        # mask为True，即real != 0时，loss不变；否则loss为0\n",
        "        loss_ *= mask\n",
        "        # return tf.reduce_mean(loss_)\n",
        "        return tf.nn.compute_average_loss(loss_, global_batch_size=global_batch_size)\n",
        "    \n",
        "    checkpoint_dir = './check'\n",
        "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "    checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                     encoder=encoder_zh2en,\n",
        "                                     decoder=decoder_zh2en)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDPghR_0R4yK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tpu_strategy.scope():\n",
        "    # 一次训练\n",
        "    def train_step(inp, targ, enc_hidden, zh2en = 1):\n",
        "        '''\n",
        "        inp: input sequence\n",
        "        targ: 翻译目标 sequence\n",
        "        enc_hidden: 编码器初始state\n",
        "\n",
        "        '''\n",
        "        if zh2en == 1:\n",
        "          encoder = encoder_zh2en\n",
        "          decoder = decoder_zh2en\n",
        "        else:\n",
        "          encoder = encoder_en2zh\n",
        "          decoder = decoder_en2zh\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "            # 编码器最后一个隐层state作为解码器第一个隐层state\n",
        "            dec_hidden = enc_hidden\n",
        "            \n",
        "            # 起始字符<s>\n",
        "            # (batch_size, 1)\n",
        "            dec_input = tf.expand_dims([word2id_zh['<s>']] * batch_size, 1)\n",
        "            \n",
        "            \n",
        "            # 教师强制，将目标词作为下一个输入\n",
        "            for t in range(1,targ.shape[1]):\n",
        "                # x, state, attention_weights\n",
        "                pre, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "                if t == 1:\n",
        "                  loss = loss_function(targ[:, t], pre)\n",
        "                loss += loss_function(targ[:, t], pre)\n",
        "                \n",
        "                # 在训练时，每次解码器的输入并不是上次解码器的输出，而是样本目标语言对应单词\n",
        "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "            # 所有单词的平均损失值\n",
        "            # targ.shape[1] = 60或40\n",
        "            batch_loss = loss / targ.shape[1]\n",
        "\n",
        "            # 模型变量包括编码器和解码器\n",
        "            variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "            # 计算损失函数关于自变量（模型参数）的梯度\n",
        "            gradients = tape.gradient(loss, variables)\n",
        "            # 根据梯度更新参数\n",
        "            optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "        return batch_loss\n",
        "\n",
        "    # 一次验证\n",
        "    def val_step(inp, targ, enc_hidden, zh2en = 1):\n",
        "        if zh2en == 1:\n",
        "            encoder = encoder_zh2en\n",
        "            decoder = decoder_zh2en\n",
        "        else:\n",
        "            encoder = encoder_en2zh\n",
        "            decoder = decoder_en2zh\n",
        "\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "        dec_hidden = enc_hidden\n",
        "        # <s> 在两个词汇表索引相同\n",
        "        dec_input = tf.expand_dims([word2id_zh['<s>']] * batch_size, 1)\n",
        "\n",
        "        # 非教师强制\n",
        "        for t in range(1,targ.shape[1]):\n",
        "            pre, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "            if t == 1:\n",
        "              loss = loss_function(targ[:, t], pre)\n",
        "            loss += loss_function(targ[:, t], pre)\n",
        "            \n",
        "            pre_argmax = tf.argmax(pre, axis = 1)\n",
        "            dec_input = tf.expand_dims(pre_argmax, 1)\n",
        "\n",
        "        # 所有单词的平均损失值\n",
        "        batch_loss = loss/targ.shape[1]\n",
        "        return batch_loss\n",
        "\n",
        "    \n",
        "    # TPU\n",
        "    # tf.distribute.ReduceOp.SUM 将各节点放缩损失相加\n",
        "    @tf.function\n",
        "    def distributed_train_step(inp, targ, enc_hidden, zh2en):\n",
        "        assert tf.distribute.get_replica_context() is None\n",
        "        per_replica_losses = tpu_strategy.experimental_run_v2(train_step, args=(inp, targ, enc_hidden))\n",
        "        return tpu_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,axis=None)\n",
        "\n",
        "    @tf.function\n",
        "    def distributed_val_step(inp, targ, enc_hidden, zh2en):\n",
        "        assert tf.distribute.get_replica_context() is None\n",
        "        per_replica_losses = tpu_strategy.experimental_run_v2(val_step, args=(inp, targ, enc_hidden))\n",
        "        return tpu_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,axis=None)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "YvSg-vFfK5K6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tpu_strategy.scope():\n",
        "    # 批训练\n",
        "    def training(zh2en):\n",
        "        if zh2en == 1:\n",
        "            encoder = encoder_zh2en\n",
        "            decoder = decoder_zh2en\n",
        "            train_dataset = train_dataset_zh2en\n",
        "            val_dataset = val_dataset_zh2en\n",
        "        else:\n",
        "            encoder = encoder_en2zh\n",
        "            decoder = decoder_en2zh\n",
        "            train_dataset = train_dataset_en2zh\n",
        "            val_dataset = val_dataset_en2zh\n",
        "\n",
        "        EPOCHS = 15\n",
        "\n",
        "        print('Start Time {}\\n'.format(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())))\n",
        "        for epoch in range(EPOCHS):\n",
        "            start = time.time()\n",
        "            # 初始化隐藏层和损失值\n",
        "            enc_hidden = encoder.initialize_hidden_state()\n",
        "            total_loss = 0.0\n",
        "\n",
        "            num_batch = 0\n",
        "            for inputs in train_dataset:\n",
        "                inp, targ = inputs\n",
        "                batch_loss = distributed_train_step(inp, targ, enc_hidden, zh2en)\n",
        "                total_loss += batch_loss\n",
        "\n",
        "                # 每50次验证集测试,并显示模型损失值\n",
        "                if num_batch % 50 == 0:\n",
        "                    total_val_loss = 0.0\n",
        "                    for x in val_dataset:\n",
        "                        inp_val, targ_val = x\n",
        "                        val_loss = distributed_val_step(inp, targ, enc_hidden, zh2en)\n",
        "                        total_val_loss += val_loss\n",
        "\n",
        "                    print('Epoch {} Batch {} Training Loss {:.4f}. Validation Loss {:.4f}.'.format(\n",
        "                                                                epoch + 1,\n",
        "                                                                num_batch,\n",
        "                                                                batch_loss,\n",
        "                                                                val_loss))\n",
        "\n",
        "                num_batch += 1\n",
        "                \n",
        "            # 每次迭代保存一次数据\n",
        "            # checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "\n",
        "            # 显示每次迭代的损失值和消耗时间\n",
        "            print('Epoch {} Average  Training Loss {:.4f}. Time {} sec.\\n'.format(epoch + 1,\n",
        "                                                total_loss / num_batch,\n",
        "                                                time.time() - start))\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "vjhuJ3hNK5K8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "bffcc62a-9bd2-4e71-f890-089afab81f73"
      },
      "source": [
        "with tpu_strategy.scope():\n",
        "    training(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Time 2020-07-13 08:11:38\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-32-664cc412d61a>:87: StrategyBase.experimental_run_v2 (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "renamed to `run`\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-32-664cc412d61a>:87: StrategyBase.experimental_run_v2 (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "renamed to `run`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Vrx7XpsJK5K-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tpu_strategy.scope():\n",
        "  from nltk.translate.bleu_score import sentence_bleu\n",
        "  import math\n",
        "  # 测试单句\n",
        "  def test_step(inp, targ, zh2en = 1):\n",
        "      if zh2en == 1:\n",
        "            encoder = encoder_zh2en\n",
        "            decoder = decoder_zh2en\n",
        "\n",
        "            # 目标\n",
        "            reference = [[id2word_en[x] for x in targ.numpy() if x != 0 and x != 2]]\n",
        "            # 原句\n",
        "            sentence = [id2word_zh[x] for x in inp.numpy() if x != 0]\n",
        "            # 注意力图 (result,sentence)\n",
        "            attention_plot = np.zeros((max_len_en, max_len_zh))\n",
        "\n",
        "            id2word = id2word_en\n",
        "      else:\n",
        "            encoder = encoder_en2zh\n",
        "            decoder = decoder_en2zh\n",
        "            reference = [[id2word_zh[x] for x in targ.numpy() if x != 0 and x != 2]]\n",
        "            sentence = [id2word_en[x] for x in inp.numpy() if x != 0]\n",
        "            attention_plot = np.zeros((max_len_zh, max_len_en))\n",
        "            id2word = id2word_zh\n",
        "\n",
        "      \n",
        "      \n",
        "      # 翻译结果\n",
        "      result = []\n",
        "\n",
        "      # No Batch\n",
        "      hidden = [tf.zeros((1, units))]\n",
        "      inputs = tf.expand_dims(inp, 0)\n",
        "\n",
        "      enc_output, enc_hidden = encoder(inputs, hidden)\n",
        "      dec_hidden = enc_hidden\n",
        "      dec_input = tf.expand_dims([word2id_zh['<s>']], 0)\n",
        "      result.append('<s>')\n",
        "\n",
        "      # 非教师强制\n",
        "      for t in range(targ.shape[0]):\n",
        "        pre, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_output)\n",
        "        # 保留注意力权重用于绘制注意力图\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "        \n",
        "        # 预测值对应字符记录在result\n",
        "        pre_id = tf.argmax(pre[0]).numpy()\n",
        "        if pre_id != 2 and pre_id != 0:\n",
        "          result.append(id2word[pre_id])\n",
        "        else:\n",
        "          break\n",
        "\n",
        "        # 将上次预测值作为下次输入\n",
        "        dec_input = tf.expand_dims([pre_id], 0)\n",
        "\n",
        "      bleu_1 = sentence_bleu(reference, result, weights=(1, 0, 0, 0))\n",
        "      bleu_2 = sentence_bleu(reference, result, weights=(0, 1, 0, 0))\n",
        "      bleu_3 = sentence_bleu(reference, result, weights=(0, 0, 1, 0))\n",
        "      bleu_4 = sentence_bleu(reference, result, weights=(0, 0, 0, 1))\n",
        "      bleu = math.exp(bleu_1 + bleu_2 + bleu_3 + bleu_4)\n",
        "\n",
        "      return result, sentence, reference, bleu, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUyqm2CdFqiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # 下载--解压--移动字体文件\n",
        "# !wget \"https://www.wfonts.com/download/data/2014/06/01/simhei/simhei.zip\"\n",
        "# !unzip \"simhei.zip\"\n",
        "# !rm \"simhei.zip\"\n",
        "# !mv SimHei.ttf /usr/share/fonts/truetype/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "JX3qayrQK5LA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tpu_strategy.scope():\n",
        "  import matplotlib.ticker as ticker\n",
        "  import matplotlib.font_manager as fm\n",
        "  path = '/usr/share/fonts/truetype/SimHei.ttf'\n",
        "  fontprop = fm.FontProperties(fname=path, size=14)\n",
        "\n",
        "  # 绘制注意力图\n",
        "  def plot_attention(attention, sentence, predicted_sentence):\n",
        "      fig = plt.figure(figsize=(len(sentence)/3,len(predicted_sentence)/3))\n",
        "      ax = fig.add_subplot(1, 1, 1)\n",
        "      ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "\n",
        "      ax.set_xticklabels([''] + sentence, rotation=90, fontproperties=fontprop)\n",
        "      ax.set_yticklabels([''] + predicted_sentence, fontproperties=fontprop)\n",
        "\n",
        "      ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "      ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "      plt.tight_layout()\n",
        "      plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Nh3XjjyPK5LD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tpu_strategy.scope():\n",
        "  bleus = []\n",
        "  print('中译英')\n",
        "  for _, (inp, targ) in enumerate(test_zh2en.take(5)):\n",
        "    res, sent, tar, bleu, atten = test_step(inp, targ, zh2en = 1)\n",
        "    bleus.append(bleu)\n",
        "    print('原句：',sent)\n",
        "    print('目标：',tar[0])\n",
        "    print('结果：',res[1:])\n",
        "    print('BLEU：',bleu)\n",
        "    plot_attention(atten[:len(res[1:]), :len(sent)], sent, res[1:])\n",
        "  print(bleus)\n",
        "  # plot_attention(attentions[0][:len(results[0]), :len(sentences[0])], sentences[0], results[0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}